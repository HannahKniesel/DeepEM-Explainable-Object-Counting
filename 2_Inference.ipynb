{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference \n",
    "Inference in deep learning is the process of using a trained model to make predictions on new, unseen data. It involves applying the learned parameters without further updates to generate outputs such as classifications or predictions.\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "from IPython.display import display\n",
    "\n",
    "from deepEM.Utils import create_text_widget\n",
    "\n",
    "from src.Inferencer import Inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Define Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference can be done on any 2D .tif files. \n",
    "Please define the path to the .tif file in `Data Path`. \n",
    "Here you can either define a directory, then inference will be done on all .tif files inside this directory, or you define the path to a single .tif file. \n",
    "\n",
    "For example usage you can define `./data/inference/`\n",
    "\n",
    "``` \n",
    "\n",
    "./data/inference/\n",
    "├── image_001.tif\n",
    "├── image_002.tif\n",
    "└── image_003.tif\n",
    "\n",
    "```\n",
    "\n",
    "Or you define `./data/inference/#17_1704_1a_2.7um_A_AC_25k_3.tif`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06dff9dfa91044f194bb7f5d48c94df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='./data/inference', description='Data Path:', style=TextStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dae5729cf19441f9f0157f65c5ff090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>Hint:</b> Enter the path to a directory which contains .tif files you wish to do inference on.'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "588e5719ed074d81912417b70ffb4907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='16', description='Batch Size:', style=TextStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e832cacad68425b9d374c79030e0a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>Hint:</b> Please set the batch size for inference. Larger batch size can lead to faster computa…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_widget = create_text_widget(\"Data Path:\",\"./data/inference\", \"Enter the path to a directory which contains .tif files you wish to do inference on.\")\n",
    "batch_widget = create_text_widget(\"Batch Size:\", 16, \"Please set the batch size for inference. Larger batch size can lead to faster computation but may lead to OOM (out of memory) errors.\")\n",
    "\n",
    "\n",
    "display(*data_widget)\n",
    "display(*batch_widget)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = data_widget[0].value\n",
    "batch_size = int(batch_widget[0].value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Choose Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff45ffd7326a46cab99a435a1db420de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Model Path:', style=TextStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97623b6f09aa4358b642f06f38fed79b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value=\"<b>Hint:</b> Enter the path to a pretrained model which you'd like to use for inference.\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_widget = create_text_widget(\"Model Path:\",\"logs/tem-herpes_2025-02-03_11-42-43/TrainingRun/checkpoints/best_model.pth\", \"Enter the path to a pretrained model which you'd like to use for inference.\")\n",
    "display(*model_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Make Prediction\n",
    "By executing the cell below, your trained model will be used to make predictions on the defined data. The predictions will be saved inside the folder of your data at `<currentdatetime>`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]::Found model checkpoint at logs/tem-herpes_2025-03-11_08-24-33/TrainingRun/checkpoints/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/DeepEM-Explainable-Object-Counting/deepEM/Inferencer.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(self.model_path)\n",
      "/teamspace/studios/this_studio/DeepEM-Explainable-Object-Counting/src/Model.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(\n",
      "/teamspace/studios/this_studio/DeepEM-Explainable-Object-Counting/deepEM/Inferencer.py:83: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(self.model_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]::Found 3 files to do inference.\n",
      "Saved prediction to ./data/inference/results-tem-herpes_2025-03-11_08-24-33/2025-03-11_08-32-15/prediction_0\n",
      "Saved prediction to ./data/inference/results-tem-herpes_2025-03-11_08-24-33/2025-03-11_08-32-15/prediction_1\n",
      "Saved prediction to ./data/inference/results-tem-herpes_2025-03-11_08-24-33/2025-03-11_08-32-15/prediction_2\n"
     ]
    }
   ],
   "source": [
    "model_path = model_widget[0].value\n",
    "inferencer = Inference(model_path, data_path)\n",
    "inferencer.inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image-to-value",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
