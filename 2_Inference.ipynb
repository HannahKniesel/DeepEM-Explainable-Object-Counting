{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference \n",
    "Inference in deep learning is the process of using a trained model to make predictions on new, unseen data. It involves applying the learned parameters without further updates to generate outputs such as classifications or predictions.\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *By executing the cell below, we import external libraries, which simplify the implementation of the notebook.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "from IPython.display import display\n",
    "\n",
    "from deepEM.Utils import create_text_widget\n",
    "\n",
    "from src.Inferencer import Inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Define Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference can be done on any 2D .tif files. \n",
    "Please define the path to the .tif file in `Data Path`. \n",
    "Here you can either define a directory, then inference will be done on all .tif files inside this directory, or you define the path to a single .tif file. \n",
    "\n",
    "For example you could define `./data/inference/` with the folder having following structure:\n",
    "\n",
    "``` \n",
    "\n",
    "./data/inference/\n",
    "├── image_001.tif\n",
    "├── image_002.tif\n",
    "└── image_003.tif\n",
    "\n",
    "```\n",
    "\n",
    "Or you define a single image: `./data/inference/#17_1704_1a_2.7um_A_AC_25k_3.tif`\n",
    "\n",
    "> *Execute the cell below to visulize a text form to provide the data for inference as well as to define the batch size.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f5016c86af64a058e89f8b4fdd105c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Data Path:', style=TextStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9403080397f7401fb51289efd6a56dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>Hint:</b> Enter the path to a directory which contains .tif files you wish to do inference on.'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f568db8dd8e42a78afe43adc93f2128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='16', description='Batch Size:', style=TextStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c278e8505d41a8ba3344e147fd7154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>Hint:</b> Please set the batch size for inference. Larger batch size can lead to faster computa…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_widget = create_text_widget(\"Data Path:\",\"\", \"Enter the path to a directory which contains .tif files you wish to do inference on.\")\n",
    "batch_widget = create_text_widget(\"Batch Size:\", 16, \"Please set the batch size for inference. Larger batch size can lead to faster computation but may lead to OOM (out of memory) errors.\")\n",
    "\n",
    "\n",
    "display(*data_widget)\n",
    "display(*batch_widget)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Execute the cell below to set the Data Path accoring to your input in the text form above.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = data_widget[0].value\n",
    "batch_size = int(batch_widget[0].value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Choose Model\n",
    "> *Execute the cell below to visulize a text form to provide the path to a trained model to do inference with.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61208c672c3c4059b49c3e1841552a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Model Path:', style=TextStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1bfc791add44d1c8818ef40a49f9bb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value=\"<b>Hint:</b> Enter the path to a pretrained model which you'd like to use for inference.\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_widget = create_text_widget(\"Model Path:\",\"\", \"Enter the path to a pretrained model which you'd like to use for inference.\")\n",
    "display(*model_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Make Prediction\n",
    "By executing the cell below, your trained model will be used to make predictions on the defined data. The predictions will be saved inside the folder of your data at `<currentdatetime>`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = model_widget[0].value\n",
    "inferencer = Inference(model_path, data_path)\n",
    "inferencer.inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image-to-value",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
