{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference \n",
    "\n",
    "\n",
    "This notebook allows you to do: \n",
    "- ✅ **Inference**: Inference refers to using a trained model to make predictions on new, unseen data. This is the final step after training and evaluation, where the model is applied to real-world data.  \n",
    "   - ✅ **Define Data**: Define the path to a directory or image which you would like to have predictions for.  \n",
    "   - ✅ **Choose Model**: Choose a trained model which you would like to use for making predictions.  \n",
    "   - ✅ **Make Predictions**: Make predictions on the provided data using the selected model.\n",
    "\n",
    "---  \n",
    "\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Execute the cell below to import external libraries, which simplify the implementation of the notebook.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "from IPython.display import display\n",
    "from deepEM.Utils import create_text_widget\n",
    "from src.Inferencer import Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Define Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference can be done on any 2D .tif files. \n",
    "Please define the path to the .tif file in `Data Path`. \n",
    "Here you can either define a directory, then inference will be done on all .tif files inside this directory, or you define the path to a single .tif file. \n",
    "\n",
    "For example you could define `./data/inference/` with the folder having following structure:\n",
    "\n",
    "``` \n",
    "\n",
    "./data/inference/\n",
    "├── image_001.tif\n",
    "├── image_002.tif\n",
    "└── image_003.tif\n",
    "\n",
    "```\n",
    "\n",
    "Or you define a single image: `./data/inference/#32_851_1b_2.5um_F_vAC_25k1.tif`\n",
    "\n",
    "> *Execute the cell below to visualize a text form to provide the data for inference as well as to define the batch size.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47cddd30fe2b47dd9dc618fdc68cab36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Data Path:', layout=Layout(width='1000px'), style=TextStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5eb3fb644c440699504ad63cc8119c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>Hint:</b> Enter the path to a directory which contains .tif files you wish to do inference on.'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6228144e90be4831b8fd9c2d24208a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='16', description='Batch Size:', layout=Layout(width='1000px'), style=TextStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "741adf3384544d05b2fa14ccaf4318c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>Hint:</b> Please set the batch size for inference. Larger batch size can lead to faster computa…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_widget = create_text_widget(\"Data Path:\",\"\", \"Enter the path to a directory which contains .tif files you wish to do inference on.\")\n",
    "batch_widget = create_text_widget(\"Batch Size:\", 16, \"Please set the batch size for inference. Larger batch size can lead to faster computation but may lead to OOM (out of memory) errors.\")\n",
    "\n",
    "display(*data_widget)\n",
    "display(*batch_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Execute the cell below to set the Data Path accoring to your input in the text form above.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = data_widget[0].value\n",
    "batch_size = int(batch_widget[0].value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Choose Model\n",
    "> *Execute the cell below to visulize a text form to provide the path to a trained model to do inference with (For example: `logs/tem-herpes_2025-02-03_11-42-43/TrainingRun/checkpoints`).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c541c2c42f840708627440e06ac2a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Model Path:', layout=Layout(width='1000px'), style=TextStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fbf2268429941feb116f9af6b82725e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value=\"<b>Hint:</b> Enter the path to a pretrained model which you'd like to use for inference (For examp…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_widget = create_text_widget(\"Model Path:\",\"\", \"Enter the path to a pretrained model which you'd like to use for inference (For example: logs/tem-herpes_2025-02-03_11-42-43/TrainingRun/checkpoints).\")\n",
    "display(*model_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Make Prediction\n",
    "> *Execute the cell below to generate predictions on the data you specified earlier using the model you defined above. Results will be stored within the provided data folder.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]::Found model checkpoint at logs/tem-herpes_2025-03-27_17-46-20/TrainingRun/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/DeepEM-Explainable-Object-Counting/deepEM/Inferencer.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(self.model_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]::Found 3 files to do inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/DeepEM-Explainable-Object-Counting/deepEM/Inferencer.py:89: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(self.model_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved prediction to ./data/inference/results-tem-herpes_2025-03-27_17-46-20/2025-04-03_12-03-38/prediction_0\n",
      "Saved prediction to ./data/inference/results-tem-herpes_2025-03-27_17-46-20/2025-04-03_12-03-38/prediction_1\n",
      "Saved prediction to ./data/inference/results-tem-herpes_2025-03-27_17-46-20/2025-04-03_12-03-38/prediction_2\n"
     ]
    }
   ],
   "source": [
    "model_path = model_widget[0].value\n",
    "inferencer = Inference(model_path, data_path)\n",
    "inferencer.inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image-to-value",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
