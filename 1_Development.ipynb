{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development \n",
    "\n",
    "## Image to Value(s)\n",
    "\n",
    "### Primary Focus: Explainable Object Counting in Microscopy Images\n",
    "### Application: Explainable Virus Capsid Quantification\n",
    "#### Challenge: Deep Learning as Black Box\n",
    "#### Required Labels: Location Labels\n",
    "\n",
    "\n",
    "TL;DR ðŸ§¬âœ¨ We developed a regression model to quantify virus capsids and their mutation stages (\"naked\" ðŸ©², \"budding\" ðŸŒ±, \"enveloped\" ðŸ“¦) during secondary envelopment in TEM images. Researchers can adapt the provided notebook within the primary focus area for their own EM data analysis (i.e. counting midrochondia).\n",
    "\n",
    "![Teaser](./images/Teaser.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from the template \n",
    "from deepEM.Utils import create_text_widget, print_info\n",
    "from deepEM.Logger import Logger\n",
    "from deepEM.ModelTuner import ModelTuner\n",
    "\n",
    "# costum implementation\n",
    "from src.ModelTrainer import ModelTrainer\n",
    "\n",
    "\n",
    "# import all required libraries\n",
    "from pathlib import Path \n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Data Acquisition\n",
    "For exemplary purposes, we utilize existing data from [1]. \n",
    "This data is stored at `./data/tem-herpes/` and can be used for test purposes. \n",
    "\n",
    "However, we strongly encourage researchers to provide their own datasets tailored to their laboratory's specific needs. This approach enables training models optimized for the lab's unique sample preparation techniques and microscope attributes, such as detector configurations. Providing a different dataset can also address different types of application, like counting of different objects within an EM image.\n",
    "\n",
    "----\n",
    "*[1] Shaga Devan, Kavitha, et al. \"Improved automatic detection of herpesvirus secondary envelopment stages in electron microscopy by augmenting training data with synthetic labelled images generated by a generative adversarial network.\" Cellular Microbiology 23.2 (2021): e13280.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Data Anntation\n",
    "\n",
    "This notebook requires annotations of object locations along with their corresponding classes.\n",
    "\n",
    "The example dataset includes annotations for the locations of virus capsids within an image, categorized by their envelopment stages, $C=[â€™nakedâ€™,â€™buddingâ€™,â€™envelopedâ€™]$.\n",
    "\n",
    "To adapt the application of this notebook, EM researchers can provide their own dataset. \n",
    "In the following, we outline an exemplary workflow for generating annotation labels for the specific task of predicting the number of virus capsids and their corresponding envelopment stages.\n",
    "\n",
    "For data annotation we recomment using the <a href=\"https://www.cvat.ai/\">CVAT</a> (Computer Vision Annotation Tool) tool. For further instructions, we refer to our <a href=\"\">Getting Started</a> page.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. Create a New Task\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://viscom-ulm.github.io/DeepEM/static/images/explainable-virus-quantification/CVAT-1.png\" alt=\"Create New Task1\" width=\"500\">\n",
    "  <br>\n",
    "</p>\n",
    "When starting CVAT, you first need to create a new task. You can give it a name, add annotation types and upload your data.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://viscom-ulm.github.io/DeepEM/static/images/explainable-virus-quantification/CVAT-2.png\" alt=\"Create New Task2\" width=\"500\">\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "Next, click on the `Add label` button. Name it based on the class you want to annotate. In our case one of *\"naked\", \"budding\", \"enveloped\"*. As annotation type choose `Points`. You should also pick a color, as this will simplify the annotation process. For adding new class click `Continue`. Once you added all nessecary classes click `Cancel`. \n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://viscom-ulm.github.io/DeepEM/static/images/explainable-virus-quantification/CVAT-3.png\" alt=\"Create New Task3\" width=\"500\">\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "Now you can upload the data you wish to annotate. Finally, click `Submit & Open` to continue with the annotation of the uploaded data. \n",
    "\n",
    "### 1.2.2. Annotation\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://viscom-ulm.github.io/DeepEM/static/images/explainable-virus-quantification/CVAT-4.png\" alt=\"Annotate1\" width=\"500\">\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "This will open following view. Click on the job (in this view the `Job #1150022`) to start the annotation job. \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://viscom-ulm.github.io/DeepEM/static/images/explainable-virus-quantification/CVAT-5.png\" alt=\"Annotate2\" width=\"500\">\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "To then annotate your data, select the `Draw new points` tool. Select the Label you wish to annotate from the dropdown menue. Then click `Shape` to annotate individual virus capsids with the label class. (Track will allow you to place annotations over multiple frames, which is helpful when annotating videos, tomograms or similar). You can use the arrows on the top middle to navigate through all of your data and to see your annotation progress. \n",
    "\n",
    "### 1.2.3. Save Annotation\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://viscom-ulm.github.io/DeepEM/static/images/explainable-virus-quantification/CVAT-6.png\" alt=\"Save1\" width=\"500\">\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "Once you are done annotating data, click on the `Menu` and select `Export job dataset`. \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://viscom-ulm.github.io/DeepEM/static/images/explainable-virus-quantification/CVAT-7.png\" alt=\"Save2\" width=\"500\">\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "During export select the `CVAT for Images 1.1` format and give the folder a name. It will prepare the dataset for download. If you have the annotated images stored locally, there is no need to enable `Save Images`. \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://viscom-ulm.github.io/DeepEM/static/images/explainable-virus-quantification/CVAT-8.png\" alt=\"Save3\" width=\"500\">\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "In the horizontal menu bar at the top go to `Requests`. It will show a request Export Annotations. On the right of this request click on the three dots on the right to download your exported, annotated data. This will download a .zip file containing the annotation file in .xml format. The name of the file should be \"annotations.xml\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Data Preprocessing\n",
    "\n",
    "The provided notebook requires all images to be in `.tif` format, containing single 2D images. If this is not the case for your data, you can use the [ImageJ](https://imagej.net/ij/) software. \n",
    "ImageJ is an open-source, Java-based image processing software that runs on multiple platforms and offers a wide range of features, including automation with macros, extensive community support, and a large library of tools and plugins.\n",
    "\n",
    "In the following we showcase an example usecase to import and export data into required formats. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./images/ImageJ-1.png\" alt=\"ImageJ1\" width=\"500\">\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "This tool allows to `import` a large amount of different, commonly used file formats im EM. \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./images/ImageJ-2.png\" alt=\"ImageJ2\" width=\"500\">\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "Using the provided `Save As..` functionality allows to save the imported files as a `Image Sequence` in .tif format or single `.tif` files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Data Structuring\n",
    "\n",
    "The provided notebook requires that all training, validation and testing data is placed within a single folder. Splitting the data into train, test and validation will be done during runtime. \n",
    "\n",
    "Additionally, the generated `annotations.xml` should be put in the same folder as the .tif images.\n",
    "\n",
    "You can check the exemplary data provided at `data/tem-herpes/` for clarification.\n",
    "\n",
    "An example with five images and the corresponding annotation is shown below: \n",
    "\n",
    "```\n",
    "/data/tem-herpes/\n",
    "â”œâ”€â”€ image_001.tif\n",
    "â”œâ”€â”€ image_002.tif\n",
    "â”œâ”€â”€ image_003.tif\n",
    "â”œâ”€â”€ image_004.tif\n",
    "â”œâ”€â”€ image_005.tif\n",
    "â””â”€â”€ annotations.xml\n",
    "\n",
    "```\n",
    "\n",
    "If you are using different data than the one provided (`data/tem-herpes/`), we require you to set the path to this folder. To do so, adapt the path in the text form below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed5afdce4624855a4a2bde869731adb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='./data/tem-herpes', description='Data Path:', style=TextStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b5ea18c31c4d8eab5bff638dad4e17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>Hint:</b> Enter the path to your data folder.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_widget = create_text_widget(\"Data Path:\",\"./data/tem-herpes\",\"Enter the path to your data folder.\")\n",
    "display(*data_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]::Data path was set to: ./data/tem-herpes\n"
     ]
    }
   ],
   "source": [
    "data_path = data_widget[0].value\n",
    "print(f\"[INFO]::Data path was set to: {data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As high resolution EM images can usually not be processed in full size by DL models, the image sizes need to be adapted. In this use case, images will be cropped to a suitable input resolution of the model (224px x 224px). For inference, the image patches will be stiched together to reconstruct the full image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Setup Logging\n",
    "\n",
    "By executing the cell below, we setup the logging directory for model training and evaluation. \n",
    "The logger creates a folder at `./logs/<datafoldername>-<currentdatetime>/`. \n",
    "Within this folder there will be logging of: \n",
    "\n",
    "- the used hyperparameters, (`<log-path>/TrainingRun/hyperparameters.json`)\n",
    "- the best performing model checkpoint based on the validation loss (`<log-path>/TrainingRun/checkpoints/best_model.pth`)\n",
    "- the last model checkpoint (`<log-path>/TrainingRun/checkpoints/latest_model.pth`)\n",
    "- visualizations of training and validation curves (`<log-path>/TrainingRun/plots/training_curves.png`)\n",
    "- qualitative visualization of sampled validation images (`<log-path>/TrainingRun/samples/`)\n",
    "- results on test metrics (`<log-path>/TrainingRun/test_results.txt`)\n",
    "- qualitative visualization of sampled test images (`<log-path>/TrainingRun/samples/`)\n",
    "\n",
    "Additional text logs are being saved to `<log-path>/TrainingRun/log.txt` and `<log-path>/log.txt` \n",
    "\n",
    "Sample visualizations of this use case include the model input, validation labels, predictions, and a GradCAM overlay. GradCAM can give an intuition about \"where the model looks\" to make its prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logger initialized. Logs will be saved to: logs/tem-herpes_2025-03-11_08-24-33\n"
     ]
    }
   ],
   "source": [
    "logger = Logger(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Hyperparameter Tuning\n",
    "\n",
    "\n",
    "Hyperparameters in deep learning are configurable settings that define how a model is trained. Unlike model parameters, they are set before training and not learned from the data.\n",
    "\n",
    "When training a model, we highly recommend to do a hyperparameter tuning first. By tuning the hyperparameters the model is usually trained on a subset of the data with a smaller number of epochs, and then evaluated based on its performance on the validation set. Then, hyperparameters, which lead to the best performance are chosen for full training of the model. \n",
    "Similar to the training run, all sweep runs will be logged. You can find the according logs at `<log-path>/Sweep-<idx>`.\n",
    "\n",
    "Our workflow equips you, as EM experts, with an automatic hyperparameter search based on a grid search. The DL experts have chosen some basic setting for performing the hyperparameter seach and defining the search space. The DL experts also describe the individual hyperparameters. This allows you to further adapt the search space to your specific needs. \n",
    "\n",
    "In order to do so, you can adapt the form below. Each sweep parameter should be separated by `,`. Floating point values should be written like `0.1`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed6be85a84e4215b8e543764fc3e908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h1>Hyperparameter Sweep</h1>                              <p>A hyperparameter sweeâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hyperparameter search\n",
    "model_trainer = ModelTrainer(data_path, logger)\n",
    "\n",
    "hyperparameter_tuner = ModelTuner(model_trainer, data_path, logger)\n",
    "form = hyperparameter_tuner.create_hyperparameter_widgets()\n",
    "display(form)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to run a hyperparameter sweep based on the parameters above, please execute the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config = None\n",
    "hyperparameter_tuner.update_config(form)\n",
    "print(\"Sweep config:\")\n",
    "for k in hyperparameter_tuner.config['hyperparameter'].keys():\n",
    "    print(f\"\\t{k}: {hyperparameter_tuner.config['hyperparameter'][k]['values']} (default: {hyperparameter_tuner.config['hyperparameter'][k]['default']})\")\n",
    "best_config = hyperparameter_tuner.tune()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our automatic hyperparameter tuning is able to find the best performing set of hyperparameters based on the setting shown above. \n",
    "\n",
    "However, there can be scenarios, where additional flexibility is required. Therefore, you are able to change these hyperparameters in the following. \n",
    "\n",
    "**WARNING** This setting is for advanced users only. Please only change parameters here, if you know what you are doing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c80580a7a9f404792c5e5218e0626f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>Warning</h2><p>Could not find best hyperparameters for current dataset (tem-herâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "form = hyperparameter_tuner.edit_hyperparameters()\n",
    "display(form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]::Will use following hyperparameters for future training: {'learning_rate': 1e-05, 'batch_size': 16}\n"
     ]
    }
   ],
   "source": [
    "best_config = hyperparameter_tuner.update_hyperparameters(form)\n",
    "print_info(f\"Will use following hyperparameters for future training: {best_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Training and Validation\n",
    "\n",
    "Training in deep learning is the process where a model learns patterns from labeled data by optimizing its parameters through backpropagation. \n",
    "Validation involves using a separate dataset to evaluate the model's performance during training, ensuring it generalizes well to unseen data.\n",
    "Hence, in this section we train and validate the model based on the provided data and hyperparameters resulting from the previous sweep.\n",
    "\n",
    "If no sweep was conducted (not recommended for new datasets!), the default parameters, defined by the DL expert will be used. \n",
    "\n",
    "In case the training run was cancelled, it can be resumed from a previous checkpoint. To do so, you need to provide a model checkpoint in the text form below. You can find these checkpoints inside the runs logging directory (`<log-dir>/TrainingRun/checkpoints/latest_model.pth`). If you do not wish to resume training, you can ignore the text form below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e11c9bd23824036bf843e3d79374a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Resume Training:', style=TextStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d10724f7f5140bf9b8a63aee2dd08bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>Hint:</b> If you wish to resume an earlier training, enter the path to the latest_model.pth filâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resume_widget = create_text_widget(\"Resume Training:\",\"\",\"If you wish to resume an earlier training, enter the path to the latest_model.pth file here.\")\n",
    "display(*resume_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 08:25:04,714 - INFO - Hyperparameters saved to logs/tem-herpes_2025-03-11_08-24-33/TrainingRun/hyperparameters.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logger initialized. Logs will be saved to: logs/tem-herpes_2025-03-11_08-24-33/TrainingRun\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/DeepEM-Explainable-Object-Counting/src/Model.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(\n",
      "2025-03-11 08:25:05,165 - INFO - Model was setup.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Dataset with 3 classes: ['naked', 'budding', 'enveloped'] and 212 micrographs.\n",
      "Setup Dataset with 3 classes: ['naked', 'budding', 'enveloped'] and 71 micrographs.\n",
      "Setup Dataset with 3 classes: ['naked', 'budding', 'enveloped'] and 71 micrographs.\n"
     ]
    }
   ],
   "source": [
    "resume_training = resume_widget[0].value\n",
    "if(resume_training):\n",
    "    resume_training = Path(resume_training)\n",
    "    if(resume_training.is_dir()):\n",
    "        resume_training = Path(os.path.join(resume_training,\"latest_model.pth\"))\n",
    "    if(not resume_training.is_file()):\n",
    "        logger.log_error(f\"Could not find resume path at {resume_training}. Will start training from scatch.\")\n",
    "        resume_training = None\n",
    "else:\n",
    "    resume_training = None\n",
    "logger.init(\"TrainingRun\")\n",
    "model_trainer.resume_from_checkpoint = resume_training\n",
    "model_trainer.prepare(best_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to train a model, execute the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 08:25:18,727 - INFO - Start Training | Epoch: 3000 | Dataset size: 212 | Parameters: {'epochs': 3000, 'early_stopping_patience': 100, 'validation_interval': 100, 'scheduler_step_by': 'epoch', 'images_to_visualize': 10, 'learning_rate': 1e-05, 'batch_size': 16} \n",
      "2025-03-11 08:25:21,724 - INFO - Epoch 0 - Training loss: 0.0651\n",
      "2025-03-11 08:25:25,544 - INFO - Saved visualizations to logs/tem-herpes_2025-03-11_08-24-33/TrainingRun/samples/validation_epoch-0_*\n",
      "2025-03-11 08:25:32,757 - INFO - Epoch 0 - Validation loss: 0.0496, MAE: 0.0320, MAE-class-naked: 0.0191, MAE-class-budding: 0.0523, MAE-class-enveloped: 0.0247\n",
      "2025-03-11 08:25:33,199 - INFO - Current model checkpoint saved to logs/tem-herpes_2025-03-11_08-24-33/TrainingRun/checkpoints/latest_model.pth\n",
      "2025-03-11 08:25:33,666 - INFO - Best model checkpoint saved to logs/tem-herpes_2025-03-11_08-24-33/TrainingRun/checkpoints/best_model.pth (Validation Loss: 0.0496)\n",
      "2025-03-11 08:25:34,130 - INFO - Saved best model checkpoint for inference to logs/tem-herpes_2025-03-11_08-24-33/TrainingRun/checkpoints/model_scripted.pt (Validation Loss: 0.0496)\n",
      "2025-03-11 08:25:34,133 - INFO - Avg time single epoch: 0h0m15s | Remaining time training: 12h33m13s\n",
      "2025-03-11 08:25:34,219 - INFO - Training curves saved to logs/tem-herpes_2025-03-11_08-24-33/TrainingRun/plots/training_curves.png\n",
      "2025-03-11 08:25:36,215 - INFO - Epoch 1 - Training loss: 0.0347\n",
      "2025-03-11 08:25:36,216 - INFO - Avg time single epoch: 0h0m8s | Remaining time training: 7h6m22s\n",
      "2025-03-11 08:25:36,301 - INFO - Training curves saved to logs/tem-herpes_2025-03-11_08-24-33/TrainingRun/plots/training_curves.png\n",
      "2025-03-11 08:25:38,264 - INFO - Epoch 2 - Training loss: 0.0417\n",
      "2025-03-11 08:25:38,265 - INFO - Avg time single epoch: 0h0m6s | Remaining time training: 5h16m49s\n",
      "2025-03-11 08:25:38,361 - INFO - Training curves saved to logs/tem-herpes_2025-03-11_08-24-33/TrainingRun/plots/training_curves.png\n",
      "2025-03-11 08:25:40,267 - INFO - Epoch 3 - Training loss: 0.0298\n",
      "2025-03-11 08:25:40,268 - INFO - Avg time single epoch: 0h0m5s | Remaining time training: 4h21m20s\n",
      "2025-03-11 08:25:40,364 - INFO - Training curves saved to logs/tem-herpes_2025-03-11_08-24-33/TrainingRun/plots/training_curves.png\n",
      "2025-03-11 08:25:42,312 - INFO - Epoch 4 - Training loss: 0.0678\n",
      "2025-03-11 08:25:42,313 - INFO - Avg time single epoch: 0h0m4s | Remaining time training: 3h48m27s\n",
      "2025-03-11 08:25:42,563 - INFO - Training curves saved to logs/tem-herpes_2025-03-11_08-24-33/TrainingRun/plots/training_curves.png\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DeepEM-Explainable-Object-Counting/deepEM/ModelTrainer.py:552\u001b[0m, in \u001b[0;36mAbstractModelTrainer.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs):\n\u001b[1;32m    551\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 552\u001b[0m     train_loss, val_loss, early_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    553\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    554\u001b[0m     elapsed_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/DeepEM-Explainable-Object-Counting/deepEM/ModelTrainer.py:435\u001b[0m, in \u001b[0;36mAbstractModelTrainer.train_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m--> 435\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/DeepEM-Explainable-Object-Counting/src/Dataset.py:302\u001b[0m, in \u001b[0;36mTrainDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    298\u001b[0m locations \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mannotations[idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoords\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    299\u001b[0m lbls \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mannotations[idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlbls\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 302\u001b[0m resized_img, resized_locations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform_resize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m cropped_img, cropped_locations, cropped_lbls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_crop(resized_img\u001b[38;5;241m.\u001b[39msqueeze(), resized_locations, lbls)\n\u001b[1;32m    304\u001b[0m cropped_img \u001b[38;5;241m=\u001b[39m cropped_img[\u001b[38;5;28;01mNone\u001b[39;00m,\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n",
      "File \u001b[0;32m~/DeepEM-Explainable-Object-Counting/src/Dataset.py:85\u001b[0m, in \u001b[0;36mRandomResizeWithLocations.__call__\u001b[0;34m(self, img, locations)\u001b[0m\n\u001b[1;32m     82\u001b[0m scale_factor \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Resize the image\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m resized_img \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscale_factor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscale_factor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Scale the locations by the same factor\u001b[39;00m\n\u001b[1;32m     88\u001b[0m resized_locations \u001b[38;5;241m=\u001b[39m locations \u001b[38;5;241m*\u001b[39m scale_factor\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torchvision/transforms/functional.py:479\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    476\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_pil\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39mpil_interpolation)\n\u001b[0;32m--> 479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mantialias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torchvision/transforms/_functional_tensor.py:465\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, antialias)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;66;03m# Define align_corners to avoid warnings\u001b[39;00m\n\u001b[1;32m    463\u001b[0m align_corners \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m interpolation \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbicubic\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malign_corners\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mantialias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m interpolation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbicubic\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m out_dtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39muint8:\n\u001b[1;32m    468\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m255\u001b[39m)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/functional.py:4077\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   4075\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m align_corners \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4076\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m antialias:\n\u001b[0;32m-> 4077\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_upsample_bilinear2d_aa\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4078\u001b[0m \u001b[38;5;66;03m# Two levels are necessary to prevent TorchScript from touching\u001b[39;00m\n\u001b[1;32m   4079\u001b[0m \u001b[38;5;66;03m# are_deterministic_algorithms_enabled.\u001b[39;00m\n\u001b[1;32m   4080\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Evaluation\n",
    "Evaluation in deep learning is the process of evaluating a trained model on a separate, unseen dataset to measure its final performance. It provides an unbiased assessment of the model's ability to generalize to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Choose Model \n",
    "\n",
    "In this section we choose the model for testing. \n",
    "If you leave the `Model Path` empty in the text form below, it will use the last model trained.\n",
    "Otherwise, you can define the path to the models best weights at `<log-path>/TrainingRun/checkpoints/best_model.pth` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cb5db2fd0624a9cb7b56f70684704b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Model Path:', style=TextStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9a4ede38204c7e91295ea7deea0914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>Hint:</b> If you wish to test a specific model, you can here define the path to its checkpoint.â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_widget = create_text_widget(\"Model Path:\",\"\",\"If you wish to test a specific model, you can here define the path to its checkpoint. (For example: logs/tem-herpes_2025-02-03_11-42-43/TrainingRun/checkpoints)\")\n",
    "display(*model_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Evaluate\n",
    "We finally evaluate the provided model on the test set. We investigate following metrics: \n",
    "\n",
    "- **Mean Absolute Error (MAE)**: A distance metric to compute the difference between the prediction and the label. Measures the absolute difference between predicted and actual counts. The lower the metric, the better. \n",
    "\n",
    "- **Mean Absolute Error (MAE-class-name)**: A distance metric to compute the difference between the prediction and the label. Measures the absolute difference between predicted and actual counts for each class individually. This can help to find a specifically well/bad performing class. The lower the metric, the better.\n",
    "\n",
    "We further visualize the input image, the model prediction and use GradCAM to highlight areas where the model was \"looking\" to make its predictions. \n",
    "These visualizations are saved to `<log-path>/TrainingRun/samples/test_*`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to evaluate a model, execute the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 08:26:09,731 - INFO - Found most recent log at logs/tem-herpes_2025-03-11_08-24-33/TrainingRun/checkpoints/best_model.pth\n",
      "/teamspace/studios/this_studio/DeepEM-Explainable-Object-Counting/deepEM/ModelTrainer.py:401: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Dataset with 3 classes: ['naked', 'budding', 'enveloped'] and 212 micrographs.\n",
      "Setup Dataset with 3 classes: ['naked', 'budding', 'enveloped'] and 71 micrographs.\n",
      "Setup Dataset with 3 classes: ['naked', 'budding', 'enveloped'] and 71 micrographs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 08:26:12,471 - INFO - Resumed training from checkpoint: logs/tem-herpes_2025-03-11_08-24-33/TrainingRun/checkpoints/best_model.pth (Validation Loss: 0.0496)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logger initialized. Logs will be saved to: logs/tem-herpes_2025-03-11_08-24-33/Evaluate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 08:26:16,417 - INFO - Saved visualizations to logs/tem-herpes_2025-03-11_08-24-33/Evaluate/samples/test_*\n",
      "2025-03-11 08:26:23,429 - INFO - Test loss: 0.0602\n",
      "2025-03-11 08:26:23,430 - INFO - MAE: 0.0398\n",
      "2025-03-11 08:26:23,430 - INFO - MAE-class-naked: 0.0283\n",
      "2025-03-11 08:26:23,431 - INFO - MAE-class-budding: 0.0630\n",
      "2025-03-11 08:26:23,431 - INFO - MAE-class-enveloped: 0.0279\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path \n",
    "start_evaluation = False\n",
    "eval_model = model_widget[0].value\n",
    "if(eval_model):\n",
    "    eval_model = Path(eval_model)\n",
    "    if(eval_model.is_dir()):\n",
    "        eval_model = Path(os.path.join(eval_model,\"best_model.pth\"))\n",
    "    if(not eval_model.is_file()):\n",
    "        logger.log_error(f\"Could not find model at {eval_model}. Make sure to train a model before evaluation.\")\n",
    "        eval_model = None\n",
    "    else: \n",
    "        start_evaluation = True\n",
    "else:\n",
    "    recent_logs = logger.get_most_recent_logs()\n",
    "    eval_model = \"\"\n",
    "    for dataname, log_path in recent_logs.items():\n",
    "        if(dataname == Path(data_path).stem):\n",
    "            eval_model = Path(log_path+\"/TrainingRun/checkpoints/best_model.pth\")\n",
    "            if(not eval_model.is_file()):\n",
    "                logger.log_error(f\"Cound not find a trained model at {eval_model}. Make sure you fully train a model first before evaluating.\")\n",
    "            else:\n",
    "                logger.log_info(f\"Found most recent log at {eval_model}\")\n",
    "                start_evaluation = True\n",
    "        else: \n",
    "            continue\n",
    "    if(not start_evaluation):\n",
    "        logger.log_error(\"Cound not find a trained model. Make sure you train a model first before evaluating.\")\n",
    "      \n",
    "if(start_evaluation):\n",
    "    model_trainer.load_checkpoint(eval_model)\n",
    "    model_trainer.test()      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image-to-value",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
